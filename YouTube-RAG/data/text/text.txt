Hey everyone. Welcome to Discover Llama Index. This is a new tutorial series that we're starting up to tell you guys all about the latest features of Llama Index and how you can really use it to supercharge your large language model applications, especially when dealing with large amounts of data. I'm Simon, the co founder and CTO of Llama Index and today I'm super excited to tell you guys just a little bit more about our latest feature, the Subquestion Query engine. Before we dive down into the details, let's recap at a high level what a query engine is in Llama Index. A query engine really is a high level natural language interface that we built over your data source so that you can very easily ask a question over your data and extract very nuanced insights out of it. Now, the key challenge in building such a unified query interface over your data is is that there's often a wide range of complicated questions that you might want to ask over multiple data sources. Now for example, you might have multiple financial documents and you want to compare and contrast the financial statements of multiple companies over multiple years. Now, just naively combining all those data sources and doing top k retrieval of chunk documents for a final synthesis is simply not good enough to get a very good and nuanced answers for complex questions. Now how do we do this? The Subquestion query engine is one step in the direction of answering these complex questions. The key intuition is that when you have a complex multi part question, the best way to deal with it is to first decompose it into multiple sub questions that are relevant to specific data sources. Now for example, here we have three data sources and we might want to decompose the question that targets a subset of them first and then combines the response over multiple data sources to get the final answer. Now this approach really leverages the composability aspect of our framework by first defining subquery engines over each of these data sources and then define a top level sub question query engine on top. Now how does this work exactly? To start, when given initial complex questions, we will use the large language model to generate sub questions given the description of our data sources that are available. Then we will execute these sub questions on the select data sources and then gather all the sub responses and then lastly we will take all the relevant context from these subsources and and synthesize the final answer. Now let's take a look at this in action to help us do some complex analysis over multiple financial documents which are called 10Ks. Okay now that we have our notebook open, we can finally go ahead and play around with some code. Before I dive down into the notebook, let me just show you the data that we'll be working with. So we have these two PDFs which are the Lyft 10K document for 2021 and the Uber document for 2021. So these are fairly large financial documents that I think have. Oh yeah, 307 pages for this one and 238 pages for this one. So quite a lot of data here and it's very heterogeneous. There are text bullet points and I think a whole bunch of text tables where a lot of financial terms are found going back to the notebook. Let's just go straight into it by doing some imports. We're going to first configure our large language model service here. We're just setting the temperature to zero so the output is more deterministic. We're using GPT3 and then basically setting the max token to negative one so that we don't limit amount of outputs. For these models. We're going to wrap the LLM predictor into our service context which we'll be using to pass into our indexing and query engine. Now for the first step we're going to use the simple directory reader to read these two files separately. So I think this might take a little bit of time since these are 300 page documents. Okay, now we have the lif 10k loaded.238 documents. I think that's correct. Yep. And let's see how long this takes. Okay, there we go. Now the next step is to build our indices here. We're just going to use the vector store index. What is happening under the hood here when we call the index from document is that each of these documents will be chunked up and into text chunks. We'll be calling the embedding API to get a dense vector embedding for each of these chunks and save it to our in memory vector store. So this take a little bit of time just because there's a lot of data. So in total 500 pages of text that needs to kind of pass through the API calls and get back to responses now. Okay, so we finished building the LIF10K index with 379 nodes. You can see that is a little bit higher than the 238 pages. I think most of the pages gets broken to one or two nodes given our default chunk size right now. Okay. Many times now just waiting for the Uber index to be built. Okay, there we go. Now we're going to build individual query engines for each of these indexes here. When we do Squery Engine, what we're really, what's really happening under the hood is that we're setting up the configuration so the query engine knows what to do at query time. Now the next step is to add sort of the metadata to identify these query engines to the top level subquestion query engine. Here we're just specifying the name of this query engine tool and also giving a description so the top level large language model can know how to make use of this data source. Okay, now that we're, we have that built, we're ready to run some queries. The first one that we're going to ask is to compare and contrast the customer segments and geographies that grew the fastest over the Lyft and Uber documents. So as we run these, you should be able to see various sub questions being generated and executed on the subquery engines before synthesizing the final answer. Okay, so we generated four sub questions and the first one is what customer segments grew the fastest for Uber? Being asked on the Uber 10K documents, you can see a some response being generated. Another question being asked for the Uber 10K about what geographies grew the fastest in 2021. The corresponding responses, the customer segments for Lyft and then the geography that grew the fastest for Lyft. So one thing to know is that I think given the context here, the model actually decided that not possible to answer this question was to give in context. Okay, now that all the sub questions have been generated, all of these question and answer pairs will be used to create a final response. And you can see, yeah, the large language model is pretty good at kind of stitching together all these information for a final synthesis that really talks about the various aspects comparing between Uber and Lyft. Now just to showcase another example, we can compare the revenue growth of Uber and Lyft from 2020 to 2021. This should be a pretty straightforward answer that really just depends on querying the right index using the right sub question. So in this case we only needed two sub questions. One is the revenue growth of Uber being asked on the Uber document and the revenue growth of Lyft being asked on the Lyft document. And as you can see, we have a clear comparison between the two as the final answer where Uber had a higher rate of revenue growth than Lyft. Cool. That's all. Hopefully you enjoyed this tutorial.